{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistilBERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNzw/Q6pMogKTIZ9BfWLRZF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cn-CHsGw59Lq","executionInfo":{"status":"ok","timestamp":1640257166708,"user_tz":-60,"elapsed":124954,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}},"outputId":"e4128970-1c8d-4f40-d6e4-1610bc94963d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/NLP Project\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/NLP\\ Project"]},{"cell_type":"code","source":["! pip install -qq transformers"],"metadata":{"id":"08SkFcL8_72Z","executionInfo":{"status":"ok","timestamp":1640258104716,"user_tz":-60,"elapsed":3719,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tqdm import tqdm\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from torch import nn\n","from torch.optim import Adam\n","from torch.utils.data import DataLoader"],"metadata":{"id":"tHv1RvXz6qu7","executionInfo":{"status":"ok","timestamp":1640263445986,"user_tz":-60,"elapsed":576,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["# Import Data"],"metadata":{"id":"dqhxd6LI9vJ3"}},{"cell_type":"code","source":["with open(\"HateXPlainData/trainHateXplain\", \"rb\") as file:\n","  train_data = pickle.load(file)\n","\n","with open(\"HateXPlainData/valHateXplain\", \"rb\") as file:\n","  val_data = pickle.load(file)\n","\n","with open(\"HateXPlainData/testHateXplain\", \"rb\") as file:\n","  test_data = pickle.load(file)"],"metadata":{"id":"7HF0nXmi6sFy","executionInfo":{"status":"ok","timestamp":1640257461680,"user_tz":-60,"elapsed":225,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(len(train_data), len(val_data), len(test_data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Ucu_ZhoMETi","executionInfo":{"status":"ok","timestamp":1640261265752,"user_tz":-60,"elapsed":217,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}},"outputId":"83a2c3fb-cebb-4214-8294-e16f49c7d93a"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["15383 1922 1924\n"]}]},{"cell_type":"markdown","source":["# Data Pre-Processing"],"metadata":{"id":"kMtaEUgS_ggY"}},{"cell_type":"code","source":["BERT_MODEL_NAME = \"distilbert-base-uncased\"\n","MAX_LEN = 100\n","BATCH_SIZE = 240\n","\n","tokenizer = DistilBertTokenizer.from_pretrained(BERT_MODEL_NAME)\n","labels = {'offensive': 2, 'hatespeech': 1, 'normal': 0}"],"metadata":{"id":"OLK8fCvF9Ad1","executionInfo":{"status":"ok","timestamp":1640262031897,"user_tz":-60,"elapsed":1185,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["class DataSet:\n","  def __init__(self, data):\n","    self.labels = []\n","    self.text = []\n","\n","    for post in data:\n","      self.labels.append(labels[post[1]])\n","      self.text.append(tokenizer.encode_plus(\" \".join(post[0]), \n","                                             add_special_tokens=True,\n","                                             truncation=True, \n","                                             max_length=MAX_LEN, \n","                                             return_token_type_ids=False, \n","                                             padding='max_length', \n","                                             return_attention_mask=True, \n","                                             return_tensors='pt'\n","                                             )\n","                      )\n","      \n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, idx):\n","    batch_texts = self.text[idx]\n","    batch_y = np.array(self.labels[idx])\n","\n","    return batch_texts, batch_y"],"metadata":{"id":"0tu9rjqeApMM","executionInfo":{"status":"ok","timestamp":1640263324410,"user_tz":-60,"elapsed":199,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["train, val, test = DataSet(train_data), DataSet(val_data), DataSet(test_data)\n","train_dataloader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n","val_dataloader = DataLoader(val, batch_size=BATCH_SIZE)\n","test_dataloader = DataLoader(test, batch_size=BATCH_SIZE)"],"metadata":{"id":"1kdgblgMS6MB","executionInfo":{"status":"ok","timestamp":1640263506094,"user_tz":-60,"elapsed":17373,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["data = next(iter(train_dataloader))\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQSkZERLYEtY","executionInfo":{"status":"ok","timestamp":1640265687629,"user_tz":-60,"elapsed":8,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}},"outputId":"7d70386d-1b62-4a90-e23a-6d9243352806"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["[{'input_ids': tensor([[[  101, 22091,  1996,  ...,     0,     0,     0]],\n","\n","        [[  101, 12461,  2024,  ...,     0,     0,     0]],\n","\n","        [[  101,  4827,  2003,  ...,     0,     0,     0]],\n","\n","        ...,\n","\n","        [[  101,  3398,  2009,  ...,     0,     0,     0]],\n","\n","        [[  101,  2043, 10643,  ...,     0,     0,     0]],\n","\n","        [[  101,  2017,  2828,  ...,     0,     0,     0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        ...,\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]],\n","\n","        [[1, 1, 1,  ..., 0, 0, 0]]])}, tensor([2, 1, 0, 1, 0, 2, 2, 2, 2, 1, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 1, 2, 0,\n","        0, 2, 2, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 2, 1,\n","        1, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1,\n","        0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 0, 1, 0, 1, 0, 2, 0,\n","        2, 2, 1, 0, 2, 1, 0, 1, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 2, 2, 0, 1, 0, 2,\n","        0, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 2, 1, 2, 0, 0, 2, 0, 0, 1, 1, 1, 1, 0,\n","        0, 0, 1, 0, 2, 0, 1, 2, 2, 1, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 1, 2, 2, 0,\n","        0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 2, 2, 0, 0, 0, 1,\n","        1, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n","        2, 0, 2, 1, 1, 0, 0, 2, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1])]\n"]}]},{"cell_type":"code","source":["model = DistilBertModel.from_pretrained(BERT_MODEL_NAME)\n","input = data[0]['input_ids'].squeeze(1)\n","mask = data[0]['attention_mask']\n","last_hidden_state, pooled_output = model(input_ids=input, attention_mask=mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"20u_dMRXbhqq","executionInfo":{"status":"error","timestamp":1640265777988,"user_tz":-60,"elapsed":60423,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}},"outputId":"d1c70ba6-113a-407f-90a7-a95af9e16c04"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-82708a2dda92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}]},{"cell_type":"markdown","source":["# Tweet Classification Model w/ DistilBERT"],"metadata":{"id":"CT-SlxlcUqW_"}},{"cell_type":"code","source":["class TweetClassifier(nn.Module):\n","  def __init__(self, num_classes):\n","    super(TweetClassifier, self).__init__()\n","\n","    self.bert = DistilBertModel.from_pretrained(BERT_MODEL_NAME)\n","    self.drop_layer = nn.Dropout(p=0.3)\n","    self.out_layer = nn.Linear(self.bert.config.hidden_size, num_classes)\n","    self.out_act = nn.Softmax(dim=1)\n","\n","  def forward(self, input, mask):\n","\n","    _, out = self.bert(input_ids=input, attention_mask=mask, return_dict=False)\n","    drop_out = self.drop_layer(out)\n","    output = self.out_layer(drop_out)\n","    return self.out_act(output)"],"metadata":{"id":"9DNU6KySToUL","executionInfo":{"status":"ok","timestamp":1640265789438,"user_tz":-60,"elapsed":219,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["tmodel = TweetClassifier(len(labels))\n","\n","tmodel(data[0]['input_ids'].squeeze(1), data[0]['attention_mask'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":531},"id":"XO9jbjKUaEEt","executionInfo":{"status":"error","timestamp":1640265841480,"user_tz":-60,"elapsed":43206,"user":{"displayName":"Hasnainali Walli","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11998737657306644499"}},"outputId":"312cec87-0ec9-4082-d400-7cfac0e39a6c"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-3ddd6126475b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-e61e84308f01>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdrop_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"]}]}]}