{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word2Vec_Embedding.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Zdaqkgy4YxknKBqF9fhMmJab_Yj5L2bc","authorship_tag":"ABX9TyMsJAercCjV75AhJUhyKceB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","%cd gdrive/MyDrive/NLP\\ Project"],"metadata":{"id":"F_GJT6dQuTdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5TBnkVMTZcs"},"outputs":[],"source":["import pickle\n","import numpy as np\n","from gensim.models import KeyedVectors"]},{"cell_type":"code","source":["## Download Word2Vec Twitter Model from here: https://mega.nz/file/h0VCxDQJ#RD11bJvp6NbEfFLGKe0H7ZGDgppz7-95LDNpep5vP2s\n","\n","w2v_vectors = KeyedVectors.load_word2vec_format(\"word2vec_twitter_model.bin\", binary=True)"],"metadata":{"id":"nUN6eZaBh-Bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open ('HateXPlainData/trainHateXplain', 'rb') as fp:\n","    train = pickle.load(fp)\n","\n","with open ('HateXPlainData/valHateXplain', 'rb') as fp:\n","    val = pickle.load(fp)\n","\n","with open ('HateXPlainData/testHateXplain', 'rb') as fp:\n","    test = pickle.load(fp)"],"metadata":{"id":"vc6hICKCtSBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def text_embed(vectors, post: list) -> np.ndarray:   \n","  embeddings = [np.zeros(vectors.vector_size)]\n","  n_tokens = 0\n","  \n","  for token in post:\n","    try:\n","      embedding = vectors[token]\n","      n_tokens += 1\n","      embeddings.append(embedding)\n","    except:\n","      continue\n","\n","  if n_tokens > 0:\n","    return sum(embeddings)/n_tokens\n","\n","  else:\n","    return sum(embeddings)"],"metadata":{"id":"woj-CvynkUmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_embeddings = np.array([text_embed(w2v_vectors, post[0]) for post in train])\n","val_embeddings = np.array([text_embed(w2v_vectors, post[0]) for post in val])\n","test_embeddings = np.array([text_embed(w2v_vectors, post[0]) for post in test])"],"metadata":{"id":"bVj0fil-0hj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.save(\"HateXPlainData/Train_W2V_Embeddings.npy\", train_embeddings)\n","np.save(\"HateXPlainData/Val_W2V_Embeddings.npy\", val_embeddings)\n","np.save(\"HateXPlainData/Test_W2V_Embeddings.npy\", test_embeddings)"],"metadata":{"id":"_-eb8b4N0jwJ"},"execution_count":null,"outputs":[]}]}